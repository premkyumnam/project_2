{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOBiyLmlFF8Pk681rysWN6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premkyumnam/demo-repo/blob/autoencoder.ipnyb/text_generation%20Using%20tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUHG-qeMIUoa"
      },
      "outputs": [],
      "source": [
        "\"\"\"Poem text generator.ipynb\n",
        "Automatically generated by Colaboratory.\n",
        "# Text Generation using LSTM\n",
        "##Two types of text generation\n",
        "1.   Character based text generation\n",
        "* Each character of the text is used to train the model and prediction will also result in generation of new characters. \n",
        "2.   Word based text generation\n",
        "* Words are converted into tokens which are used to train the model. The model will generate words instead of characters in the prediction stage.\n",
        "## Mechanics of the text generation model: \n",
        "1. The next word of the sequence is predicted using the words that are already present in the sequence. \n",
        "2. It is a simple model where splitting the data into training and testing sets is not required. This is because the model will use all the words in the sequence to predict the next word. Just like forecasting.\n",
        "#The flow of program:\n",
        "1. Loading data\n",
        "2. Preprocessing the data and Tokenizing \n",
        "3. Building and fitting the model on data\n",
        "4. Evaluate the model\n",
        "5. Predicting(Generating the text)\n",
        "6. Saving the model for future applicaitons\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting the device to read the text"
      ],
      "metadata": {
        "id": "9lmjJM1VPjY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GSyKMwPqIdaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mKiIif07Ipfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data from local google drive \n",
        "with open('/content/drive/My Drive/poem.txt') as story:\n",
        "  story_data = story.read()\n",
        "\n",
        "print(story_data)"
      ],
      "metadata": {
        "id": "ov5C0bY4I4n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data cleaning process\n",
        "import re                                # Regular expressions to use sub function for replacing the useless text from the data\n"
      ],
      "metadata": {
        "id": "Pmi7L-GKKPKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub(r',', '', text)\n",
        "  text = re.sub(r'\\'', '',  text)\n",
        "  text = re.sub(r'\\\"', '', text)\n",
        "  text = re.sub(r'\\(', '', text)\n",
        "  text = re.sub(r'\\)', '', text)\n",
        "  text = re.sub(r'\\n', '', text)\n",
        "  text = re.sub(r'“', '', text)\n",
        "  text = re.sub(r'”', '', text)\n",
        "  text = re.sub(r'’', '', text)\n",
        "  text = re.sub(r'\\.', '', text)\n",
        "  text = re.sub(r';', '', text)\n",
        "  text = re.sub(r':', '', text)\n",
        "  text = re.sub(r'\\-', '', text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "WVJRGE0uKSM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning the data\n",
        "lower_data = story_data.lower()           # Converting the string to lower case to get uniformity\n",
        "\n",
        "split_data = lower_data.splitlines()      # Splitting the data to get every line seperately but this will give the list of uncleaned data\n",
        "\n",
        "print(split_data)                         "
      ],
      "metadata": {
        "id": "P8DcMP1UKvWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = ''                                # initiating a argument with blank string to hold the values of final cleaned data\n"
      ],
      "metadata": {
        "id": "OIFuNLjgK1Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in split_data:\n",
        "  line = clean_text(line)\n",
        "  final += '\\n' + line\n",
        "\n",
        "print(final)"
      ],
      "metadata": {
        "id": "hIlOK2_4K5B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = final.split('\\n')       # splitting again to get list of cleaned and splitted data ready to be processed\n",
        "print(final_data)"
      ],
      "metadata": {
        "id": "COUgykoFK_SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "eVu42GneLAwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the Tokenizer\n",
        "max_vocab = 1000000\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(final_data)\n"
      ],
      "metadata": {
        "id": "JOGCZhtCLI4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the total number of words of the data.\n",
        "word2idx = tokenizer.word_index\n",
        "print(len(word2idx))\n",
        "print(word2idx)\n",
        "vocab_size = len(word2idx) + 1        # Adding 1 to the vocab_size because the index starts from 1 not 0. This will make it uniform when using it further\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "id": "JhBGy_42LMGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## Creating n-gram sequences from the sentences\n",
        "* Consider this sentence : ['two roads diverged in a yellow wood']. Here we will use ['two roads diverged in a yellow'] to predict ['wood']. This is the basic concept of forecasting which can be applied here to generate text.\n",
        "* An advacement of this will be to use single word or every combination words possible from the sentence to predict the next word. And this is loosely termed as n_gram sequences\n",
        "* The sentence ['two roads diverged in a yellow wood'] will have sequence as [112, 113, 114, 7, 5, 190, 75]\n",
        "* so we will use combinations of words to make our model better\n",
        "* [112, 113], \n",
        "* [112, 113, 114], \n",
        "* [112, 113, 114, 7], \n",
        "* [112, 113, 114, 7, 5], \n",
        "* [112, 113, 114, 7, 5, 190], \n",
        "* [112, 113, 114, 7, 5, 190, 75]\n",
        "* we train our model that if 112 comes then it has to predict 113.\n",
        "* if combination of 112, 113, comes then it has to predict 114 and so on.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cQwu_ggPLSsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will turn the sentences to sequences line by line and create n_gram sequences\n",
        "\n",
        "input_seq = []\n",
        "\n",
        "for line in final_data:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_seq = token_list[:i+1]\n",
        "    input_seq.append(n_gram_seq)\n",
        "\n",
        "print(input_seq)"
      ],
      "metadata": {
        "id": "JRt89UGULctT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the maximum length of sequence for padding purpose\n",
        "max_seq_length = max(len(x) for x in input_seq)\n",
        "print(max_seq_length)\n"
      ],
      "metadata": {
        "id": "ntmL7M2JLkv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding the sequences and converting them to array\n",
        "input_seq = np.array(pad_sequences(input_seq, maxlen=max_seq_length, padding='pre'))\n",
        "print(input_seq)"
      ],
      "metadata": {
        "id": "FG_lixyRLpRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking xs and labels to train the model\n",
        "xs = input_seq[:, :-1]        # xs contains every word in sentence except the last one because we are using this value to predict the y value\n",
        "labels = input_seq[:, -1]     # labels contains only the last word of the sentence which will help in hot encoding the y value in next step\n",
        "print(\"xs: \",xs)\n",
        "print(\"labels:\",labels)"
      ],
      "metadata": {
        "id": "1UrBOy3NLs5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "Ty_TdOT7NK8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding the labels according to the vocab size\n",
        "\n",
        "# The matrix is square matrix of the size of vocab_size. Each row will denote a label and it will have \n",
        "# a single +ve value(i.e 1) for that label and other values will be zero. \n",
        "\n",
        "ys = to_categorical(labels, num_classes=vocab_size)\n",
        "print(ys)"
      ],
      "metadata": {
        "id": "ys_JTy1gNSMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Bidirectional, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "metadata": {
        "id": "zTSyPmWFNVqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the functional APIs of keras to define the model\n",
        "\n",
        "i = Input(shape=(max_seq_length - 1, ))                           # using 1 less value becasuse we are preserving the last value for predicted word \n",
        "x = Embedding(vocab_size, 124)(i)\n",
        "x = Dropout(0.2)(x)\n",
        "x = LSTM(520, return_sequences=True)(x)\n",
        "x = Bidirectional(layer=LSTM(340, return_sequences=True))(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "model = Model(i,x)"
      ],
      "metadata": {
        "id": "NQ91LpecNaw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the pipeline method of sequential to define a model\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocab_size, 124, input_length=max_seq_length-1))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(520, return_sequences=True))\n",
        "# model.add(Bidirectional(LSTM(340, return_sequences=True)))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "# model.add(Dense(1024, activation='relu'))\n",
        "# model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5qf0REPiNliw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()                                       # We can know about the shape of the model\n",
        "\n",
        "r = model.fit(xs,ys,epochs=100)"
      ],
      "metadata": {
        "id": "nSyCHYaZNyoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "54XbJMdwN_KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(r.history['accuracy'])"
      ],
      "metadata": {
        "id": "cLyN-BEpPnwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NliGpbZtR0mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to take input of seed text from user and no. of words to be predicted\n",
        "\n",
        "def predict_words(seed, no_words):\n",
        "  for i in range(no_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=1)\n"
      ],
      "metadata": {
        "id": "4OY1KFBzPsbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_word=''"
      ],
      "metadata": {
        "id": "GP-Tlo1WSC44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting or generating the poem with the seed text\n",
        "\n",
        "seed_text = 'i am feeling good today'\n",
        "next_words = 20\n",
        "\n",
        "predict_words(seed_text, next_words)"
      ],
      "metadata": {
        "id": "HA1JZj9mP6GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "model.save('poem_generator.h5')  # will create a HDFS file of the model"
      ],
      "metadata": {
        "id": "IQY2rJXnSHKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_word"
      ],
      "metadata": {
        "id": "pEUE7xjATMPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vITPWHyJTPzc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}